{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyXKj4YHOZdi"
      },
      "outputs": [],
      "source": [
        "!git clone "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WGbTgb0EIzL",
        "outputId": "3a97f53c-c918-4d1a-da7c-3fbbc85a4688"
      },
      "outputs": [],
      "source": [
        "!pip install transformers; pip install wayback; pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OuUpKJ9w019",
        "outputId": "d62fa233-b816-48b4-f6cb-e7b9f80bb320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-25 17:03:19--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_test.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.207, 142.251.2.207, 142.250.101.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29264 (29K) [application/vnd.ms-excel]\n",
            "Saving to: ‘finance_test.csv’\n",
            "\n",
            "\rfinance_test.csv      0%[                    ]       0  --.-KB/s               \rfinance_test.csv    100%[===================>]  28.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-25 17:03:19 (106 MB/s) - ‘finance_test.csv’ saved [29264/29264]\n",
            "\n",
            "--2024-04-25 17:03:19--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_train.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.207, 142.251.2.207, 142.250.101.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 258593 (253K) [application/vnd.ms-excel]\n",
            "Saving to: ‘finance_train.csv’\n",
            "\n",
            "finance_train.csv   100%[===================>] 252.53K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-04-25 17:03:19 (93.4 MB/s) - ‘finance_train.csv’ saved [258593/258593]\n",
            "\n",
            "Train and Test Files Loaded as train.csv and test.csv\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "from wayback import WaybackClient\n",
        "import gdown\n",
        "import torch\n",
        "import numpy as np\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "from urllib.request import Request\n",
        "\n",
        "# gdown.download('https://drive.google.com/uc?id=1q4U2gVY9tWEPdT6W-pdQpKmo152QqWLE', 'finance_train.csv', True)\n",
        "# gdown.download('https://drive.google.com/uc?id=1nIBqAsItwVEGVayYTgvybz7HeK0asom0', 'finance_test.csv', True)\n",
        "\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_test.csv'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_train.csv'\n",
        "\n",
        "n = 20 #the # of article headlines displayed per ticker\n",
        "tickers = ['AAPL', 'TSLA', 'AMZN']\n",
        "\n",
        "def get_finance_train():\n",
        "  df_train = pd.read_csv(\"finance_train.csv\")\n",
        "  return df_train\n",
        "def get_finance_test():\n",
        "  df_test = pd.read_csv(\"finance_test.csv\")\n",
        "  return df_test\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "print (\"Train and Test Files Loaded as train.csv and test.csv\")\n",
        "\n",
        "LABEL_MAP = {0 : \"negative\", 1 : \"neutral\", 2 : \"positive\"}\n",
        "NONE = 4 * [None]\n",
        "RND_SEED=2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT1-FyK4EtoV"
      },
      "outputs": [],
      "source": [
        "msft = yf.Ticker(\"MSFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u0gM3QbFO4F"
      },
      "outputs": [],
      "source": [
        "hist = msft.history(period=\"1mo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0kdwdeAFWfU",
        "outputId": "68c7ffe1-3357-481e-f2cb-4ce0629c296c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'currency': 'USD',\n",
              " 'symbol': 'MSFT',\n",
              " 'exchangeName': 'NMS',\n",
              " 'fullExchangeName': 'NasdaqGS',\n",
              " 'instrumentType': 'EQUITY',\n",
              " 'firstTradeDate': 511108200,\n",
              " 'regularMarketTime': 1714064599,\n",
              " 'hasPrePostMarketData': True,\n",
              " 'gmtoffset': -14400,\n",
              " 'timezone': 'EDT',\n",
              " 'exchangeTimezoneName': 'America/New_York',\n",
              " 'regularMarketPrice': 395.68,\n",
              " 'fiftyTwoWeekHigh': 395.74,\n",
              " 'fiftyTwoWeekLow': 388.035,\n",
              " 'regularMarketDayHigh': 395.74,\n",
              " 'regularMarketDayLow': 388.035,\n",
              " 'regularMarketVolume': 20890844,\n",
              " 'chartPreviousClose': 422.86,\n",
              " 'priceHint': 2,\n",
              " 'currentTradingPeriod': {'pre': {'timezone': 'EDT',\n",
              "   'start': 1714032000,\n",
              "   'end': 1714051800,\n",
              "   'gmtoffset': -14400},\n",
              "  'regular': {'timezone': 'EDT',\n",
              "   'start': 1714051800,\n",
              "   'end': 1714075200,\n",
              "   'gmtoffset': -14400},\n",
              "  'post': {'timezone': 'EDT',\n",
              "   'start': 1714075200,\n",
              "   'end': 1714089600,\n",
              "   'gmtoffset': -14400}},\n",
              " 'dataGranularity': '1d',\n",
              " 'range': '1mo',\n",
              " 'validRanges': ['1d',\n",
              "  '5d',\n",
              "  '1mo',\n",
              "  '3mo',\n",
              "  '6mo',\n",
              "  '1y',\n",
              "  '2y',\n",
              "  '5y',\n",
              "  '10y',\n",
              "  'ytd',\n",
              "  'max']}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "msft.history_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q-98iM_FRUl",
        "outputId": "34ffe0ab-2036-4a8a-d7aa-c8213a4c6092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 Open        High         Low       Close  \\\n",
            "Date                                                                        \n",
            "2024-03-25 00:00:00-04:00  425.239990  427.410004  421.609985  422.859985   \n",
            "2024-03-26 00:00:00-04:00  425.609985  425.989990  421.350006  421.649994   \n",
            "2024-03-27 00:00:00-04:00  424.440002  424.450012  419.010010  421.429993   \n",
            "2024-03-28 00:00:00-04:00  420.959991  421.869995  419.119995  420.720001   \n",
            "2024-04-01 00:00:00-04:00  423.950012  427.890015  422.220001  424.570007   \n",
            "2024-04-02 00:00:00-04:00  420.109985  422.380005  417.839996  421.440002   \n",
            "2024-04-03 00:00:00-04:00  419.730011  423.260010  419.089996  420.450012   \n",
            "2024-04-04 00:00:00-04:00  424.989990  428.670013  417.570007  417.880005   \n",
            "2024-04-05 00:00:00-04:00  420.010010  426.510010  418.320007  425.519989   \n",
            "2024-04-08 00:00:00-04:00  425.170013  427.279999  423.299988  424.589996   \n",
            "2024-04-09 00:00:00-04:00  426.440002  427.739990  421.619995  426.279999   \n",
            "2024-04-10 00:00:00-04:00  422.190002  424.029999  419.700012  423.260010   \n",
            "2024-04-11 00:00:00-04:00  425.820007  429.369995  422.359985  427.929993   \n",
            "2024-04-12 00:00:00-04:00  424.049988  425.179993  419.769989  421.899994   \n",
            "2024-04-15 00:00:00-04:00  426.600006  426.820007  413.429993  413.640015   \n",
            "2024-04-16 00:00:00-04:00  414.570007  418.399994  413.730011  414.579987   \n",
            "2024-04-17 00:00:00-04:00  417.250000  418.880005  410.329987  411.839996   \n",
            "2024-04-18 00:00:00-04:00  410.630005  411.890015  403.950012  404.269989   \n",
            "2024-04-19 00:00:00-04:00  404.029999  405.480011  397.769989  399.119995   \n",
            "2024-04-22 00:00:00-04:00  400.079987  402.850006  395.750000  400.959991   \n",
            "2024-04-23 00:00:00-04:00  404.239990  408.200012  403.059998  407.570007   \n",
            "2024-04-24 00:00:00-04:00  409.559998  412.470001  406.779999  409.059998   \n",
            "2024-04-25 00:00:00-04:00  409.559998  395.739990  388.035492  395.679993   \n",
            "\n",
            "                             Volume  Dividends  Stock Splits  \n",
            "Date                                                          \n",
            "2024-03-25 00:00:00-04:00  18060500        0.0           0.0  \n",
            "2024-03-26 00:00:00-04:00  16725600        0.0           0.0  \n",
            "2024-03-27 00:00:00-04:00  16705000        0.0           0.0  \n",
            "2024-03-28 00:00:00-04:00  21871200        0.0           0.0  \n",
            "2024-04-01 00:00:00-04:00  16316000        0.0           0.0  \n",
            "2024-04-02 00:00:00-04:00  17912000        0.0           0.0  \n",
            "2024-04-03 00:00:00-04:00  16502300        0.0           0.0  \n",
            "2024-04-04 00:00:00-04:00  19370900        0.0           0.0  \n",
            "2024-04-05 00:00:00-04:00  16544300        0.0           0.0  \n",
            "2024-04-08 00:00:00-04:00  14272400        0.0           0.0  \n",
            "2024-04-09 00:00:00-04:00  12512300        0.0           0.0  \n",
            "2024-04-10 00:00:00-04:00  16216600        0.0           0.0  \n",
            "2024-04-11 00:00:00-04:00  17966400        0.0           0.0  \n",
            "2024-04-12 00:00:00-04:00  19232100        0.0           0.0  \n",
            "2024-04-15 00:00:00-04:00  20273500        0.0           0.0  \n",
            "2024-04-16 00:00:00-04:00  16765600        0.0           0.0  \n",
            "2024-04-17 00:00:00-04:00  15855500        0.0           0.0  \n",
            "2024-04-18 00:00:00-04:00  21029900        0.0           0.0  \n",
            "2024-04-19 00:00:00-04:00  30276500        0.0           0.0  \n",
            "2024-04-22 00:00:00-04:00  20286900        0.0           0.0  \n",
            "2024-04-23 00:00:00-04:00  15734500        0.0           0.0  \n",
            "2024-04-24 00:00:00-04:00  14997700        0.0           0.0  \n",
            "2024-04-25 00:00:00-04:00  20890844        0.0           0.0  \n"
          ]
        }
      ],
      "source": [
        "print(hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhzuX54C2cjV"
      },
      "source": [
        "## Webscrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "sXqV5hrC06FJ",
        "outputId": "3d5fa87e-bace-446f-811a-6bdc86f01df8"
      },
      "outputs": [
        {
          "ename": "HTTPError",
          "evalue": "HTTP Error 403: Forbidden",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-67c4a290444c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinviz_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'user-agent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'my-app/0.0.1'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnews_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'news-table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ],
      "source": [
        "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
        "news_tables = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    url = finviz_url + ticker\n",
        "    req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'})\n",
        "    resp = urlopen(req)\n",
        "    html = BeautifulSoup(resp, features=\"lxml\")\n",
        "    news_table = html.find(id='news-table')\n",
        "    news_tables[ticker] = news_table\n",
        "\n",
        "try:\n",
        "    for ticker in tickers:\n",
        "        df = news_tables[ticker]\n",
        "        df_tr = df.findAll('tr')\n",
        "\n",
        "\n",
        "        for i, table_row in enumerate(df_tr):\n",
        "            a_text = table_row.a.text\n",
        "            td_text = table_row.td.text\n",
        "            td_text = td_text.strip()\n",
        "            if i == n-1:\n",
        "                break\n",
        "except KeyError:\n",
        "    pass\n",
        "\n",
        "parsed_news = []\n",
        "for file_name, news_table in news_tables.items():\n",
        "    for x in news_table.findAll('tr'):\n",
        "        text = x.a.get_text()\n",
        "        date_scrape = x.td.text.split()\n",
        "\n",
        "        if len(date_scrape) == 1:\n",
        "            time = date_scrape[0]\n",
        "\n",
        "        else:\n",
        "            date = date_scrape[0]\n",
        "            time = date_scrape[1]\n",
        "\n",
        "        ticker = file_name.split('_')[0]\n",
        "\n",
        "        parsed_news.append([ticker, date, time, text])\n",
        "\n",
        "columns = ['Ticker', 'Date', 'Time', 'Headline']\n",
        "news = pd.DataFrame(parsed_news, columns=columns)\n",
        "print(news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq9nGrP72fsc"
      },
      "source": [
        "## Tokenize stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftTFmftD1X-c"
      },
      "outputs": [],
      "source": [
        "df_train = get_finance_train()\n",
        "df_test = get_finance_test()\n",
        "sentences = df_train[\"Sentence\"].values\n",
        "labels = df_train[\"Label\"].values\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case = True)\n",
        "sentences_with_special_tokens = []\n",
        "tokenized_texts = []\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "  sentences_with_special_tokens.append(sentence)\n",
        "\n",
        "print(sentences_with_special_tokens[0])\n",
        "\n",
        "for sentence in sentences_with_special_tokens:\n",
        "  sentence = tokenizer.tokenize(sentence)\n",
        "  tokenized_texts.append(sentence)\n",
        "\n",
        "print(tokenized_texts[0])\n",
        "\n",
        "\n",
        "for sentence in tokenized_texts:\n",
        "  sentence = tokenizer.convert_tokens_to_ids(sentence)\n",
        "  input_ids.append(sentence)\n",
        "print(input_ids[0])\n",
        "input_ids = pad_sequences(input_ids,\n",
        "                          maxlen=128,\n",
        "                          dtype=\"long\",\n",
        "                          truncating=\"post\",\n",
        "                          padding=\"post\")\n",
        "print(input_ids[0])\n",
        "\n",
        "\n",
        "\n",
        "for sentence in input_ids:\n",
        "  sentence = [float(i>0) for i in sentence]\n",
        "  attention_masks.append(sentence)\n",
        "\n",
        "print(attention_masks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W7H5zeB2jMr"
      },
      "source": [
        "## Splits, Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqaNvgqd2DoQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(input_ids, labels, test_size = 0.15, random_state=RND_SEED)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, test_size = 0.15, random_state=RND_SEED)\n",
        "train_inputs = torch.tensor(np.array(X_train));\n",
        "validation_inputs = torch.tensor(np.array(X_val));\n",
        "train_masks = torch.tensor(np.array(train_masks));\n",
        "validation_masks = torch.tensor(np.array(validation_masks));\n",
        "train_labels = torch.tensor(np.array(y_train));\n",
        "validation_labels = torch.tensor(np.array(y_val));\n",
        "\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels);\n",
        "train_sampler = RandomSampler(train_data);\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size);\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels);\n",
        "validation_sampler = SequentialSampler(validation_data);\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size);\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ");\n",
        "\n",
        "\n",
        "model.cuda();\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "epochs = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6G6G8W2ob7"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jON0cnXU1h-v"
      },
      "outputs": [],
      "source": [
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "training_stats = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training the model')\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "\n",
        "    print(\"Evaluating on Validation Set\")\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    print(\"Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "\n",
        "    training_loss.append(avg_train_loss)\n",
        "    validation_loss.append(avg_val_loss)\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy\n",
        "\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgpaTsWf1jKi"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.title('Loss over Time')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.plot(training_loss, label = \"train\")\n",
        "plt.plot(validation_loss, label = \"val_loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "test_sentences = df_test[\"Sentence\"].values\n",
        "test_labels = df_test[\"Label\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itN-MAw01lPe"
      },
      "outputs": [],
      "source": [
        "test_input_ids, test_attention_masks = [], []\n",
        "\n",
        "test_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]\n",
        "\n",
        "tokenized_test_sentences = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
        "\n",
        "test_input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_test_sentences]\n",
        "\n",
        "test_input_ids = pad_sequences(test_input_ids,\n",
        "                               maxlen=128,\n",
        "                               dtype=\"long\",\n",
        "                               truncating=\"post\",\n",
        "                               padding=\"post\")\n",
        "\n",
        "for sequence in test_input_ids:\n",
        "  mask = [float(i>0) for i in sequence]\n",
        "  test_attention_masks.append(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXp27ndq1mOQ"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "test_input_ids = torch.tensor(test_input_ids)\n",
        "test_attention_masks = torch.tensor(test_attention_masks)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUGUpyTk1nXC"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "\n",
        "print ('Test Accuracy: {:.2%}'.format(flat_accuracy(logits, label_ids)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gKwGCXq1pA_"
      },
      "outputs": [],
      "source": [
        "real_input_ids, real_attention_masks = [], []\n",
        "\n",
        "real_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in news[\"Headline\"]]\n",
        "\n",
        "tokenized_real_sentences = [tokenizer.tokenize(sent) for sent in real_sentences]\n",
        "\n",
        "real_input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_real_sentences]\n",
        "\n",
        "real_input_ids = pad_sequences(real_input_ids,\n",
        "                               maxlen=128,\n",
        "                               dtype=\"long\",\n",
        "                               truncating=\"post\",\n",
        "                               padding=\"post\")\n",
        "\n",
        "for sequence in test_input_ids:\n",
        "  mask = [float(i>0) for i in sequence]\n",
        "  real_attention_masks.append(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5bqGtc61sVP"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "real_input_ids = torch.tensor(real_input_ids)\n",
        "real_attention_masks = torch.tensor(real_attention_masks)\n",
        "prediction_data = TensorDataset(real_input_ids, real_attention_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6W8D-yf1tnG"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "print(outputs)\n",
        "df_scores = pd.DataFrame(outputs)\n",
        "news = news.join(df_scores, rsuffix='_right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ1uB6ne1wDQ"
      },
      "outputs": [],
      "source": [
        "news['Date'] = pd.to_datetime(news.Date).dt.date\n",
        "\n",
        "unique_ticker = news['Ticker'].unique().tolist()\n",
        "news_dict = {name: news.loc[news['Ticker'] == name] for name in unique_ticker}\n",
        "\n",
        "values = []\n",
        "for ticker in tickers:\n",
        "    dataframe = news_dict[ticker]\n",
        "    dataframe = dataframe.set_index('Ticker')\n",
        "    dataframe = dataframe.drop(columns = ['Headline'])\n",
        "    print ('\\n')\n",
        "    print (dataframe.head())\n",
        "\n",
        "    mean = round(dataframe['compound'].mean(), 2)\n",
        "    values.append(mean)\n",
        "\n",
        "df = pd.DataFrame(list(zip(tickers, values)), columns =['Ticker', 'Mean Sentiment'])\n",
        "df = df.set_index('Ticker')\n",
        "df = df.sort_values('Mean Sentiment', ascending=False)\n",
        "print ('\\n')\n",
        "print (df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
