{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (5.1.0)\n",
            "Requirement already satisfied: matplotlib in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.8.4)\n",
            "Requirement already satisfied: torch in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: pandas in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: transformers in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.40.1)\n",
            "Requirement already satisfied: wayback in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.4.5)\n",
            "Requirement already satisfied: yfinance in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.2.38)\n",
            "Requirement already satisfied: keras in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (3.3.2)\n",
            "Collecting tensorflow (from -r requirements.txt (line 10))\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (4.12.3)\n",
            "Requirement already satisfied: filelock in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.21 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (24.0)\n",
            "Requirement already satisfied: pillow>=8 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2024.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 4)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 4)) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.13.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 6)) (0.22.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 6)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 6)) (2024.4.16)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 6)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 6)) (0.4.3)\n",
            "Requirement already satisfied: urllib3>=1.20 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from wayback->-r requirements.txt (line 7)) (2.2.1)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from yfinance->-r requirements.txt (line 8)) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from yfinance->-r requirements.txt (line 8)) (5.2.1)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from yfinance->-r requirements.txt (line 8)) (1.4.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from yfinance->-r requirements.txt (line 8)) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from yfinance->-r requirements.txt (line 8)) (3.17.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from yfinance->-r requirements.txt (line 8)) (1.1)\n",
            "Requirement already satisfied: absl-py in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from keras->-r requirements.txt (line 9)) (2.1.0)\n",
            "Requirement already satisfied: rich in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from keras->-r requirements.txt (line 9)) (13.7.1)\n",
            "Requirement already satisfied: namex in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from keras->-r requirements.txt (line 9)) (0.0.8)\n",
            "Requirement already satisfied: h5py in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from keras->-r requirements.txt (line 9)) (3.11.0)\n",
            "Requirement already satisfied: optree in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from keras->-r requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from keras->-r requirements.txt (line 9)) (0.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting ml-dtypes (from keras->-r requirements.txt (line 9))\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (20 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 10)) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from tensorflow->-r requirements.txt (line 10)) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading grpcio-1.62.2-cp310-cp310-macosx_12_0_universal2.whl.metadata (4.0 kB)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 10)) (0.43.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 1)) (2.5)\n",
            "Requirement already satisfied: webencodings in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from html5lib>=1.1->yfinance->-r requirements.txt (line 8)) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (2024.2.2)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 10))\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow->-r requirements.txt (line 10))\n",
            "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from rich->keras->-r requirements.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from rich->keras->-r requirements.txt (line 9)) (2.17.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/benjaminli/opt/miniconda3/envs/stocks_stats/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->-r requirements.txt (line 9)) (0.1.2)\n",
            "Downloading tensorflow-2.16.1-cp310-cp310-macosx_12_0_arm64.whl (227.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.0/227.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.62.2-cp310-cp310-macosx_12_0_universal2.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-macosx_10_9_universal2.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-macosx_12_0_arm64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading wrapt-1.16.0-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
            "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Downloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, ml-dtypes, markdown, grpcio, google-pasta, gast, astunparse, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.2 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 opt-einsum-3.3.0 protobuf-4.25.3 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 werkzeug-3.0.2 wrapt-1.16.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers; pip install wayback; pip install yfinance\n",
        "!pip install -r requirements.txt\n",
        "# !git clone https://github.com/25benjaminli/stocks_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lyXKj4YHOZdi"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "from wayback import WaybackClient\n",
        "import gdown\n",
        "import torch\n",
        "import numpy as np\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "from urllib.request import Request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OuUpKJ9w019",
        "outputId": "d62fa233-b816-48b4-f6cb-e7b9f80bb320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dyld[92945]: Library not loaded: /opt/homebrew/opt/libunistring/lib/libunistring.2.dylib\n",
            "  Referenced from: <98BA8F4C-DC8D-31D6-8402-5484E5D2FC2A> /opt/homebrew/Cellar/wget/1.21.3/bin/wget\n",
            "  Reason: tried: '/opt/homebrew/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/opt/homebrew/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/usr/local/lib/libunistring.2.dylib' (no such file), '/usr/lib/libunistring.2.dylib' (no such file, not in dyld cache), '/opt/homebrew/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/opt/homebrew/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/usr/local/lib/libunistring.2.dylib' (no such file), '/usr/lib/libunistring.2.dylib' (no such file, not in dyld cache)\n",
            "dyld[92948]: Library not loaded: /opt/homebrew/opt/libunistring/lib/libunistring.2.dylib\n",
            "  Referenced from: <98BA8F4C-DC8D-31D6-8402-5484E5D2FC2A> /opt/homebrew/Cellar/wget/1.21.3/bin/wget\n",
            "  Reason: tried: '/opt/homebrew/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/opt/homebrew/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/usr/local/lib/libunistring.2.dylib' (no such file), '/usr/lib/libunistring.2.dylib' (no such file, not in dyld cache), '/opt/homebrew/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/opt/homebrew/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/usr/local/lib/libunistring.2.dylib' (no such file), '/usr/lib/libunistring.2.dylib' (no such file, not in dyld cache)\n",
            "Train and Test Files Loaded as train.csv and test.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'currency': 'USD',\n",
              " 'symbol': 'MSFT',\n",
              " 'exchangeName': 'NMS',\n",
              " 'fullExchangeName': 'NasdaqGS',\n",
              " 'instrumentType': 'EQUITY',\n",
              " 'firstTradeDate': 511108200,\n",
              " 'regularMarketTime': 1714066498,\n",
              " 'hasPrePostMarketData': True,\n",
              " 'gmtoffset': -14400,\n",
              " 'timezone': 'EDT',\n",
              " 'exchangeTimezoneName': 'America/New_York',\n",
              " 'regularMarketPrice': 396.109,\n",
              " 'fiftyTwoWeekHigh': 396.565,\n",
              " 'fiftyTwoWeekLow': 388.035,\n",
              " 'regularMarketDayHigh': 396.565,\n",
              " 'regularMarketDayLow': 388.035,\n",
              " 'regularMarketVolume': 22492271,\n",
              " 'chartPreviousClose': 422.86,\n",
              " 'priceHint': 2,\n",
              " 'currentTradingPeriod': {'pre': {'timezone': 'EDT',\n",
              "   'end': 1714051800,\n",
              "   'start': 1714032000,\n",
              "   'gmtoffset': -14400},\n",
              "  'regular': {'timezone': 'EDT',\n",
              "   'end': 1714075200,\n",
              "   'start': 1714051800,\n",
              "   'gmtoffset': -14400},\n",
              "  'post': {'timezone': 'EDT',\n",
              "   'end': 1714089600,\n",
              "   'start': 1714075200,\n",
              "   'gmtoffset': -14400}},\n",
              " 'dataGranularity': '1d',\n",
              " 'range': '1mo',\n",
              " 'validRanges': ['1d',\n",
              "  '5d',\n",
              "  '1mo',\n",
              "  '3mo',\n",
              "  '6mo',\n",
              "  '1y',\n",
              "  '2y',\n",
              "  '5y',\n",
              "  '10y',\n",
              "  'ytd',\n",
              "  'max']}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# gdown.download('https://drive.google.com/uc?id=1q4U2gVY9tWEPdT6W-pdQpKmo152QqWLE', 'finance_train.csv', True)\n",
        "# gdown.download('https://drive.google.com/uc?id=1nIBqAsItwVEGVayYTgvybz7HeK0asom0', 'finance_test.csv', True)\n",
        "\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_test.csv'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_train.csv'\n",
        "\n",
        "n = 20 #the # of article headlines displayed per ticker\n",
        "tickers = ['AAPL', 'TSLA', 'AMZN']\n",
        "\n",
        "def get_finance_train():\n",
        "  df_train = pd.read_csv(\"finance_train.csv\")\n",
        "  return df_train\n",
        "def get_finance_test():\n",
        "  df_test = pd.read_csv(\"finance_test.csv\")\n",
        "  return df_test\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "print (\"Train and Test Files Loaded as train.csv and test.csv\")\n",
        "\n",
        "LABEL_MAP = {0 : \"negative\", 1 : \"neutral\", 2 : \"positive\"}\n",
        "NONE = 4 * [None]\n",
        "RND_SEED=2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhzuX54C2cjV"
      },
      "source": [
        "## Webscrape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq9nGrP72fsc"
      },
      "source": [
        "## Tokenize stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftTFmftD1X-c"
      },
      "outputs": [],
      "source": [
        "df_train = get_finance_train()\n",
        "df_test = get_finance_test()\n",
        "sentences = df_train[\"Sentence\"].values\n",
        "labels = df_train[\"Label\"].values\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case = True)\n",
        "sentences_with_special_tokens = []\n",
        "tokenized_texts = []\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "  sentences_with_special_tokens.append(sentence)\n",
        "\n",
        "print(sentences_with_special_tokens[0])\n",
        "\n",
        "for sentence in sentences_with_special_tokens:\n",
        "  sentence = tokenizer.tokenize(sentence)\n",
        "  tokenized_texts.append(sentence)\n",
        "\n",
        "print(tokenized_texts[0])\n",
        "\n",
        "\n",
        "for sentence in tokenized_texts:\n",
        "  sentence = tokenizer.convert_tokens_to_ids(sentence)\n",
        "  input_ids.append(sentence)\n",
        "print(input_ids[0])\n",
        "input_ids = pad_sequences(input_ids,\n",
        "                          maxlen=128,\n",
        "                          dtype=\"long\",\n",
        "                          truncating=\"post\",\n",
        "                          padding=\"post\")\n",
        "print(input_ids[0])\n",
        "\n",
        "\n",
        "\n",
        "for sentence in input_ids:\n",
        "  sentence = [float(i>0) for i in sentence]\n",
        "  attention_masks.append(sentence)\n",
        "\n",
        "print(attention_masks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W7H5zeB2jMr"
      },
      "source": [
        "## Splits, Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqaNvgqd2DoQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(input_ids, labels, test_size = 0.15, random_state=RND_SEED)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, test_size = 0.15, random_state=RND_SEED)\n",
        "train_inputs = torch.tensor(np.array(X_train));\n",
        "validation_inputs = torch.tensor(np.array(X_val));\n",
        "train_masks = torch.tensor(np.array(train_masks));\n",
        "validation_masks = torch.tensor(np.array(validation_masks));\n",
        "train_labels = torch.tensor(np.array(y_train));\n",
        "validation_labels = torch.tensor(np.array(y_val));\n",
        "\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels);\n",
        "train_sampler = RandomSampler(train_data);\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size);\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels);\n",
        "validation_sampler = SequentialSampler(validation_data);\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size);\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ");\n",
        "\n",
        "\n",
        "model.cuda();\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "epochs = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6G6G8W2ob7"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jON0cnXU1h-v"
      },
      "outputs": [],
      "source": [
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "training_stats = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training the model')\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "\n",
        "    print(\"Evaluating on Validation Set\")\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    print(\"Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "\n",
        "    training_loss.append(avg_train_loss)\n",
        "    validation_loss.append(avg_val_loss)\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy\n",
        "\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgpaTsWf1jKi"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.title('Loss over Time')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.plot(training_loss, label = \"train\")\n",
        "plt.plot(validation_loss, label = \"val_loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "test_sentences = df_test[\"Sentence\"].values\n",
        "test_labels = df_test[\"Label\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itN-MAw01lPe"
      },
      "outputs": [],
      "source": [
        "test_input_ids, test_attention_masks = [], []\n",
        "\n",
        "test_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]\n",
        "\n",
        "tokenized_test_sentences = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
        "\n",
        "test_input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_test_sentences]\n",
        "\n",
        "test_input_ids = pad_sequences(test_input_ids,\n",
        "                               maxlen=128,\n",
        "                               dtype=\"long\",\n",
        "                               truncating=\"post\",\n",
        "                               padding=\"post\")\n",
        "\n",
        "for sequence in test_input_ids:\n",
        "  mask = [float(i>0) for i in sequence]\n",
        "  test_attention_masks.append(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXp27ndq1mOQ"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "test_input_ids = torch.tensor(test_input_ids)\n",
        "test_attention_masks = torch.tensor(test_attention_masks)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUGUpyTk1nXC"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "\n",
        "print ('Test Accuracy: {:.2%}'.format(flat_accuracy(logits, label_ids)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gKwGCXq1pA_"
      },
      "outputs": [],
      "source": [
        "real_input_ids, real_attention_masks = [], []\n",
        "\n",
        "real_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in news[\"Headline\"]]\n",
        "\n",
        "tokenized_real_sentences = [tokenizer.tokenize(sent) for sent in real_sentences]\n",
        "\n",
        "real_input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_real_sentences]\n",
        "\n",
        "real_input_ids = pad_sequences(real_input_ids,\n",
        "                               maxlen=128,\n",
        "                               dtype=\"long\",\n",
        "                               truncating=\"post\",\n",
        "                               padding=\"post\")\n",
        "\n",
        "for sequence in test_input_ids:\n",
        "  mask = [float(i>0) for i in sequence]\n",
        "  real_attention_masks.append(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5bqGtc61sVP"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "real_input_ids = torch.tensor(real_input_ids)\n",
        "real_attention_masks = torch.tensor(real_attention_masks)\n",
        "prediction_data = TensorDataset(real_input_ids, real_attention_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6W8D-yf1tnG"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "print(outputs)\n",
        "df_scores = pd.DataFrame(outputs)\n",
        "news = news.join(df_scores, rsuffix='_right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ1uB6ne1wDQ"
      },
      "outputs": [],
      "source": [
        "news['Date'] = pd.to_datetime(news.Date).dt.date\n",
        "\n",
        "unique_ticker = news['Ticker'].unique().tolist()\n",
        "news_dict = {name: news.loc[news['Ticker'] == name] for name in unique_ticker}\n",
        "\n",
        "values = []\n",
        "for ticker in tickers:\n",
        "    dataframe = news_dict[ticker]\n",
        "    dataframe = dataframe.set_index('Ticker')\n",
        "    dataframe = dataframe.drop(columns = ['Headline'])\n",
        "    print ('\\n')\n",
        "    print (dataframe.head())\n",
        "\n",
        "    mean = round(dataframe['compound'].mean(), 2)\n",
        "    values.append(mean)\n",
        "\n",
        "df = pd.DataFrame(list(zip(tickers, values)), columns =['Ticker', 'Mean Sentiment'])\n",
        "df = df.set_index('Ticker')\n",
        "df = df.sort_values('Mean Sentiment', ascending=False)\n",
        "print ('\\n')\n",
        "print (df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
