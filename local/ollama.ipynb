{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\", cache=False, top_p = 0.9, top_k = 40, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You didn't ask me anything yet. This conversation just started, and your first message was an empty line. Would you like to ask me something now?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv(\"../sentiment_score.csv\")\n",
    "constituents = pd.read_csv(\"../s&p_constituents.csv\")\n",
    "names = [i.lower() for i in constituents['Security'].tolist()]\n",
    "tickers = constituents['Symbol'].tolist()\n",
    "\n",
    "current_ticker = \"MSFT\"\n",
    "for headline in df['headline']:\n",
    "    st = \"{\\\"TSLA\\\": \\\"positive\\\", \\\"AAPL\\\": \\\"neutral\\\", \\\"GOOG\\\": \\\"negative\\\"} or {} if no companies mentioned\"\n",
    "    prompt = f\"\"\"\n",
    "    The following financial news headline is about {current_ticker} but may mention other companies.\n",
    "    Please provide the sentiment (positive, neutral, or negative) solely in relation to {current_ticker} \n",
    "    given the following headline in the brackets below. The sentiment should reflect the favorability of {current_ticker} for investors. \n",
    "    If you are not sure, answer neutral.\n",
    "    [{headline}]\n",
    "    \"\"\".strip()\n",
    "    # Omit any explanation of sentiment analysis.\n",
    "    # You are a financial analyst with knowledge of all stock tickers in the s&p 500. \n",
    "    # for chunks in llm.stream(prompt):\n",
    "    #     print(chunks, end=\"\")\n",
    "    val = llm.invoke(prompt)\n",
    "    # use regex to parse for dictionary, denoted by curly braces\n",
    "    print(\"**HEADLINE**\")\n",
    "    print(headline)\n",
    "    print(\"**RESPONSE**\")\n",
    "    print(val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# read from finvizurls.txt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from datetime import date, datetime\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "class HistoricalSentiment:\n",
    "\n",
    "    def __init__(self, ticker, fn=vader.polarity_scores):\n",
    "        self.ticker = ticker\n",
    "        self.fn = fn\n",
    "\n",
    "    def find_articles(self, url):\n",
    "        url_req = f\"{url}/quote.ashx?t={self.ticker}\"\n",
    "        req = Request(url=url_req, headers={\"User-Agent\": \"FireFox\"}) # I realize that aditya's version of the code doesn't use the right user agent\n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response, \"html.parser\")\n",
    "        news_table = html.find(id='news-table')\n",
    "\n",
    "        return news_table\n",
    "    \n",
    "    def generate_news_df(self, news_table):\n",
    "        news_list = []\n",
    "        # oldest.datetime_timestamp\n",
    "        # datetime.datetime(1998, 11, 11, 18, 45, 51)\n",
    "\n",
    "        # TODO: filter based on time (i.e. use previous day to get news for next day)\n",
    "\n",
    "        for i in news_table.findAll('tr'):\n",
    "            try:\n",
    "                text = i.a.get_text()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            date_scrape = i.td.text.split()\n",
    "            source = i.div.span.get_text()\n",
    "\n",
    "            if len(date_scrape) == 1:\n",
    "                time = date_scrape[0]\n",
    "\n",
    "            else:\n",
    "                final_date = date_scrape[0]\n",
    "                time = date_scrape[1]\n",
    "\n",
    "                if final_date == \"Today\":\n",
    "                    final_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            tick = self.ticker\n",
    "\n",
    "            news_list.append([tick, final_date, time, source, text])\n",
    "\n",
    "        columns = ['ticker', 'date', 'time', 'source', 'headline']\n",
    "        news_df = pd.DataFrame(news_list, columns=columns)\n",
    "        news_df['date'] = pd.to_datetime(news_df.date, format='mixed').dt.date\n",
    "\n",
    "        # randomly select 40 headlines from 40 different days. This will have to be stratified by date\n",
    "        # don't necessarily select 40 randomly, just take all\n",
    "        # for i in range(40):\n",
    "        #     news_df = news_df.sample(frac=1).groupby('date').head(1)\n",
    "        print(\"length of news df\", len(news_df))\n",
    "\n",
    "        return news_df\n",
    "    \n",
    "    def calculate_sentiment(self, url):\n",
    "        self.news_scraped = self.find_articles(url=url)\n",
    "        self.news_df = self.generate_news_df(self.news_scraped)\n",
    "        # requires that find_articles has been called and generated a news_df\n",
    "\n",
    "        scores = self.news_df['headline'].apply(self.fn).tolist()\n",
    "        scores = [x['compound'] for x in scores]\n",
    "        sentiment = float(np.mean(scores))\n",
    "        final_sentiment = round(sentiment, 4)\n",
    "        # print(self.news_df.head())\n",
    "        return self.news_df['headline'], final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finvizurls_test.txt\", \"r\") as f:\n",
    "    urls_select = f.readlines()\n",
    "\n",
    "# urls_select = random.sample(range(len(urls)), 40)\n",
    "# urls_select = [urls[i] for i in urls_select]\n",
    "news_tables = {}\n",
    "\n",
    "tickers = [\"MSFT\"]\n",
    "\n",
    "print(\"urls selected\", len(urls_select))\n",
    "\n",
    "sentiments = {}\n",
    "for url in urls_select:\n",
    "    # save url to access metadata\n",
    "    # beatiful soup to extract the text\n",
    "    url = url.strip()\n",
    "    for ticker in tickers:\n",
    "        obj_vader = HistoricalSentiment(ticker, vader.polarity_scores)\n",
    "        headline, sentiment_vader = obj_vader.calculate_sentiment(url=url)\n",
    "        print(\"headlines:\")\n",
    "        print(headline.head())\n",
    "        print(\"aggregated sentiment:\", sentiment_vader)\n",
    "        sentiments[ticker] = sentiment_vader\n",
    "        break\n",
    "\n",
    "print(\"sentiments\", sentiments)\n",
    "\n",
    "\n",
    "# TODO: investigate after hours stock moving - how do we deal with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your job is to determine the sentiment (positive, negative, neutral) corresponding\n",
    "    to companies and their stock tickers, if any, explicitly mentioned in the given financial news headline. \n",
    "    Only determine sentiment corresponding to companies explicitly mentioned in the \n",
    "    headline, do not try to predict companies that might be in the article. \n",
    "    Print only what belongs in the braces. \n",
    "    Examples of outputs include {st}. Do not explain the output.\n",
    "    If you are not sure, please don't share false information. \n",
    "\n",
    "    Headline: [Meet the Supercharged Growth Stock That's a Shoo-in to Join Microsoft in the $3 Trillion Club]\n",
    "    Predicted sentiment: {\"{}\"}\n",
    "    \n",
    "    di = re.search(r'\\{.*\\}', val).group()\n",
    "    # if the dictionary is not in the correct format, ensure the keys and values have quotes\n",
    "    # convert any company names to corresponding ticker\n",
    "    for name_idx, name in enumerate(names):\n",
    "        for key in di:\n",
    "            # if the key is a company name, replace it with the corresponding ticker\n",
    "            # name will be multiple words long. if any part of the name is in the key, replace it\n",
    "            for word in name.split():\n",
    "                if word.lower() in key:\n",
    "                    di = di.replace(key, tickers[name_idx])\n",
    "    # remove any quotes around the keys\n",
    "    # di = re.sub(r'\"', '', di)\n",
    "    di = re.sub(r'(\\w+):', r'\"\\1\":', di)\n",
    "\n",
    "    print(\"di original\", di)\n",
    "    di_real = eval(di)\n",
    "    print(headline, di_real)\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks_stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
