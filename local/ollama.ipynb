{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_startend_from_date(date):\n",
    "    # what kind of monstrosity did I create here\n",
    "    reformatted_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    date_start = datetime.strptime((reformatted_date - timedelta(days=7)).strftime(\"%Y%m%d\"), \"%Y%m%d\")\n",
    "    date_end = datetime.strptime((reformatted_date - timedelta(days=1)).strftime(\"%Y%m%d\"), \"%Y%m%d\")\n",
    "    # print(\"starting, ending days\", date_start, date_end, datetime.strftime(reformatted_date, \"%Y%m%d\"))\n",
    "    return date_start, date_end, datetime.strftime(reformatted_date, \"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_overlap(date, startenddates):\n",
    "    idx_matches = []\n",
    "    for idx, startend in enumerate(startenddates):\n",
    "        if date >= startend[0] and date <= startend[1]:\n",
    "            idx_matches.append(idx)\n",
    "    \n",
    "    # if len(idx_matches) > 1:\n",
    "    #     print(\"Multiple matches for date\", idx_matches)\n",
    "    return idx_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://web.archive.org/web/20220301001745/https://finviz.com/\n",
    "df = pd.read_csv(\"pct_change.csv\")\n",
    "dates_using = df[\"Date\"].tolist()\n",
    "startenddates = []\n",
    "real_dates = []\n",
    "\n",
    "for idx, date in enumerate(dates_using):\n",
    "    # calculate the boundaries that it would be valid\n",
    "    date_start, date_end, reformatted_date = get_startend_from_date(date)\n",
    "    # print(\"starting, ending days\", date_start, date_end, reformatted_date)\n",
    "    startenddates.append([date_start, date_end])\n",
    "    real_dates.append(reformatted_date)\n",
    "\n",
    "idx_lens = set()\n",
    "\n",
    "with open(\"finvizurls_dates.txt\", \"r\") as f:\n",
    "    urls = f.readlines()\n",
    "    for url in urls:\n",
    "\n",
    "        # get the date\n",
    "        date_url = url[28:42]\n",
    "        # date_url = datetime.strftime(\"%Y%m%d\")\n",
    "        date_url = datetime.strptime(date_url, \"%Y%m%d%H%M%S\")\n",
    "        # only get the year, month, date\n",
    "        date_url = datetime.strptime(date_url.strftime(\"%Y%m%d\"), \"%Y%m%d\")\n",
    "        # print(date_url)\n",
    "\n",
    "        for idx, date in enumerate(dates_using):\n",
    "            date_start, date_end = startenddates[idx]\n",
    "            reformatted_date = real_dates[idx]\n",
    "            if date_url >= date_start and date_url <= date_end:\n",
    "                df.at[idx, \"start\"] = date_start\n",
    "                df.at[idx, \"end\"] = date_end\n",
    "                df.at[idx, \"actual\"] = reformatted_date\n",
    "\n",
    "        for idx, date in enumerate(dates_using):\n",
    "            # print(date)\n",
    "            # print(date_url, startenddates[idx], real_dates[idx])\n",
    "            idx_matches = check_for_overlap(date_url, startenddates)\n",
    "            # print(\"idx matches for\", date_url, idx_matches)\n",
    "            # for idx_match in idx_matches:\n",
    "            #     print(\"idx match\", startenddates[idx_match])\n",
    "            idx_lens.add(len(idx_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(startenddates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct = pd.read_csv(\"pct_change.csv\")\n",
    "# sort pct by date\n",
    "pct = pct.sort_values(by=\"Date\")\n",
    "# save\n",
    "# pct.to_csv(\"pct_change_sort.csv\", index=False)\n",
    "# check for duplicates in pct by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>percent_change</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>-2.808480</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>2022-03-06</td>\n",
       "      <td>20220307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-10</td>\n",
       "      <td>-0.508713</td>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>2023-08-09</td>\n",
       "      <td>20230810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-10-07</td>\n",
       "      <td>-1.034414</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>2014-10-06</td>\n",
       "      <td>20141007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-05</td>\n",
       "      <td>-0.182060</td>\n",
       "      <td>2019-10-29</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>20191105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>-1.279243</td>\n",
       "      <td>2017-08-10</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>20170817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  percent_change      start        end    actual\n",
       "0  2022-03-07       -2.808480 2022-02-28 2022-03-06  20220307\n",
       "1  2023-08-10       -0.508713 2023-08-03 2023-08-09  20230810\n",
       "2  2014-10-07       -1.034414 2014-09-30 2014-10-06  20141007\n",
       "3  2019-11-05       -0.182060 2019-10-29 2019-11-04  20191105\n",
       "4  2017-08-17       -1.279243 2017-08-10 2017-08-16  20170817"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\", cache=False, top_p = 0.9, top_k = 40, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You didn't ask me anything yet. This conversation just started, and your first message was an empty line. Would you like to ask me something now?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv(\"../sentiment_score.csv\")\n",
    "constituents = pd.read_csv(\"../s&p_constituents.csv\")\n",
    "names = [i.lower() for i in constituents['Security'].tolist()]\n",
    "tickers = constituents['Symbol'].tolist()\n",
    "\n",
    "current_ticker = \"MSFT\"\n",
    "for headline in df['headline']:\n",
    "    st = \"{\\\"TSLA\\\": \\\"positive\\\", \\\"AAPL\\\": \\\"neutral\\\", \\\"GOOG\\\": \\\"negative\\\"} or {} if no companies mentioned\"\n",
    "    prompt = f\"\"\"\n",
    "    The following financial news headline is about {current_ticker} but may mention other companies.\n",
    "    Please provide the sentiment (positive, neutral, or negative) solely in relation to {current_ticker} \n",
    "    given the following headline in the brackets below. The sentiment should reflect the favorability of {current_ticker} for investors. \n",
    "    If you are not sure, answer neutral.\n",
    "    [{headline}]\n",
    "    \"\"\".strip()\n",
    "    # Omit any explanation of sentiment analysis.\n",
    "    # You are a financial analyst with knowledge of all stock tickers in the s&p 500. \n",
    "    # for chunks in llm.stream(prompt):\n",
    "    #     print(chunks, end=\"\")\n",
    "    val = llm.invoke(prompt)\n",
    "    # use regex to parse for dictionary, denoted by curly braces\n",
    "    print(\"**HEADLINE**\")\n",
    "    print(headline)\n",
    "    print(\"**RESPONSE**\")\n",
    "    print(val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\25ben\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# read from finvizurls.txt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from datetime import date, datetime\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "import urllib.parse\n",
    "import posixpath\n",
    "\n",
    "class HistoricalSentiment:\n",
    "\n",
    "    def __init__(self, ticker=None, fn=vader.polarity_scores):\n",
    "        self.ticker = ticker\n",
    "        self.fn = fn\n",
    "\n",
    "    def find_articles(self, url):\n",
    "        # url_req = f\"{url}/quote.ashx\" # ?t={self.ticker}\n",
    "        # url_req = urllib.parse.urljoin(url, f\"quote.ashx?t={self.ticker}\")\n",
    "        url_req = posixpath.join(url, f\"quote.ashx?t={self.ticker}\")\n",
    "        print(url_req)\n",
    "\n",
    "        req = Request(url=url_req, headers={\"User-Agent\": \"FireFox\"}) # I realize that aditya's version of the code doesn't use the right user agent\n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response, \"html.parser\")\n",
    "        news_table = html.find(id='news-table') # id\n",
    "        news_tablev2 = html.find(class_='fullview-news-outer')\n",
    "        # return whichever table is not None\n",
    "        print(\"using news table\" if news_table is not None else \"using news table v2\")\n",
    "        \n",
    "        return news_table if news_table is not None else news_tablev2\n",
    "    \n",
    "    def generate_news_df(self, news_table):\n",
    "        news_list = []\n",
    "        # oldest.datetime_timestamp\n",
    "        # datetime.datetime(1998, 11, 11, 18, 45, 51)\n",
    "\n",
    "        # TODO: filter based on time (i.e. use previous day to get news for next day)\n",
    "\n",
    "        for i in news_table.findAll('tr'):\n",
    "            try:\n",
    "                text = i.a.get_text()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            date_scrape = i.td.text.split()\n",
    "            source = i.div.span.get_text()\n",
    "\n",
    "            if len(date_scrape) == 1:\n",
    "                time = date_scrape[0]\n",
    "\n",
    "            else:\n",
    "                final_date = date_scrape[0]\n",
    "                time = date_scrape[1]\n",
    "\n",
    "                if final_date == \"Today\":\n",
    "                    final_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            tick = self.ticker\n",
    "\n",
    "            news_list.append([tick, final_date, time, source, text])\n",
    "\n",
    "        columns = ['ticker', 'date', 'time', 'source', 'headline']\n",
    "        news_df = pd.DataFrame(news_list, columns=columns)\n",
    "        news_df['date'] = pd.to_datetime(news_df.date, format='mixed').dt.date\n",
    "\n",
    "        # randomly select 40 headlines from 40 different days. This will have to be stratified by date\n",
    "        # don't necessarily select 40 randomly, just take all\n",
    "        # for i in range(40):\n",
    "        #     news_df = news_df.sample(frac=1).groupby('date').head(1)\n",
    "        print(\"length of news df\", len(news_df))\n",
    "\n",
    "        return news_df\n",
    "    \n",
    "    def calculate_sentiment(self, urls):\n",
    "        final_df = \n",
    "        for url in urls:\n",
    "            self.news_scraped = self.find_articles(url=url)\n",
    "            self.news_df = self.generate_news_df(self.news_scraped)\n",
    "            # requires that find_articles has been called and generated a news_df\n",
    "\n",
    "        scores = self.news_df['headline'].apply(self.fn).tolist()\n",
    "        scores = [x['compound'] for x in scores]\n",
    "        sentiment = float(np.mean(scores))\n",
    "        final_sentiment = round(sentiment, 4)\n",
    "        # print(self.news_df.head())\n",
    "        return self.news_df['headline'], final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_articles(url, ticker):\n",
    "    # url_req = f\"{url}/quote.ashx\" # ?t={self.ticker}\n",
    "    url_req = posixpath.join(url, f\"quote.ashx?t={ticker}\")\n",
    "    print(url_req)\n",
    "\n",
    "    req = Request(url=url_req, headers={\"User-Agent\": \"FireFox\"}) # I realize that aditya's version of the code doesn't use the right user agent\n",
    "    response = urlopen(req)\n",
    "    html = BeautifulSoup(response, \"html.parser\")\n",
    "    news_table = html.find(id='news-table') # id\n",
    "    news_tablev2 = html.find(class_='fullview-news-outer')\n",
    "    # return whichever table is not None\n",
    "    print(\"using news table\" if news_table is not None else \"using news table v2\")\n",
    "    \n",
    "    return news_table if news_table is not None else news_tablev2\n",
    "\n",
    "def generate_news_df(ticker, news_table):\n",
    "    news_list = []\n",
    "    # TODO: filter based on time (i.e. use previous day to get news for next day)\n",
    "\n",
    "    for i in news_table.findAll('tr'):\n",
    "        try:\n",
    "            text = i.a.get_text()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        date_scrape = i.td.text.split()\n",
    "        source = i.div.span.get_text()\n",
    "\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "\n",
    "        else:\n",
    "            final_date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "            if final_date == \"Today\":\n",
    "                final_date = date.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "            llm_date = check_for_overlap(final_date, startenddates)\n",
    "\n",
    "        news_list.append([ticker, time, source, text, final_date, llm_date])\n",
    "\n",
    "    columns = ['ticker', 'time', 'source', 'headline', 'date', 'llm_date']\n",
    "    news_df = pd.DataFrame(news_list, columns=columns)\n",
    "    news_df['date'] = pd.to_datetime(news_df.date, format='mixed').dt.date\n",
    "\n",
    "    print(\"length of news df\", len(news_df))\n",
    "\n",
    "    return news_df\n",
    "\n",
    "def calculate_sentiment(urls):\n",
    "    final_df = pd.DataFrame()\n",
    "    for url in urls:\n",
    "        news_scraped = find_articles(url=url)\n",
    "        news_df = generate_news_df(news_scraped)\n",
    "        # merge with final_df but only keep unique headlines (i.e. don't keep duplicates)\n",
    "        final_df = pd.concat([final_df, news_df])\n",
    "\n",
    "    final_df = final_df.drop_duplicates(subset='headline')\n",
    "\n",
    "    scores = final_df['headline'].apply(vader.polarity_scores).tolist()\n",
    "    scores = [x['compound'] for x in scores]\n",
    "    sentiment = float(np.mean(scores))\n",
    "    final_sentiment = round(sentiment, 4)\n",
    "    # print(final_df.head())\n",
    "    print(\"number of unique headlines\", len(final_df))\n",
    "    return final_df['headline'], final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls selected 676\n",
      "=== URL 0 ===\n",
      "https://web.archive.org/web/20220228005514/http://finviz.com/quote.ashx?t=MSFT\n",
      "length of news df 100\n",
      "aggregated sentiment for MSFT: 0.0007\n",
      "=== URL 1 ===\n",
      "https://web.archive.org/web/20220301001745/https://finviz.com/quote.ashx?t=MSFT\n",
      "length of news df 100\n",
      "aggregated sentiment for MSFT: 0.0007\n",
      "=== URL 2 ===\n",
      "https://web.archive.org/web/20220302011012/http://finviz.com/quote.ashx?t=MSFT\n",
      "length of news df 100\n",
      "aggregated sentiment for MSFT: 0.0007\n",
      "=== URL 3 ===\n",
      "https://web.archive.org/web/20220303051854/http://www.finviz.com/quote.ashx?t=MSFT\n",
      "length of news df 100\n",
      "aggregated sentiment for MSFT: 0.0007\n",
      "=== URL 4 ===\n",
      "https://web.archive.org/web/20220304194101/https://finviz.com/quote.ashx?t=MSFT\n",
      "length of news df 100\n",
      "aggregated sentiment for MSFT: 0.0007\n",
      "=== URL 5 ===\n",
      "https://web.archive.org/web/20230803010522/https://finviz.com/quote.ashx?t=MSFT\n",
      "length of news df 100\n",
      "aggregated sentiment for MSFT: 0.1444\n",
      "=== URL 6 ===\n",
      "https://web.archive.org/web/20230804033501/http://finviz.com/quote.ashx?t=MSFT\n",
      "length of news df 100\n",
      "aggregated sentiment for MSFT: 0.1444\n",
      "=== URL 7 ===\n",
      "https://web.archive.org/web/20230805035203/http://finviz.com/quote.ashx?t=MSFT\n",
      "length of news df 100\n",
      "aggregated sentiment for MSFT: 0.1444\n",
      "=== URL 8 ===\n",
      "https://web.archive.org/web/20230806052940/http://finviz.com/quote.ashx?t=MSFT\n",
      "length of news df 100\n",
      "aggregated sentiment for MSFT: 0.1444\n",
      "=== URL 9 ===\n",
      "https://web.archive.org/web/20230807004353/https://finviz.com/quote.ashx?t=MSFT\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:1354\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1354\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\http\\client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\http\\client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1301\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\http\\client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1251\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\http\\client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1011\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1014\u001b[0m \n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\http\\client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\http\\client.py:1418\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1418\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\http\\client.py:922\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39msetsockopt(socket\u001b[38;5;241m.\u001b[39mIPPROTO_TCP, socket\u001b[38;5;241m.\u001b[39mTCP_NODELAY, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\socket.py:808\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\socket.py:796\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    795\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 796\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers:\n\u001b[0;32m     20\u001b[0m     obj_vader \u001b[38;5;241m=\u001b[39m HistoricalSentiment(ticker, vader\u001b[38;5;241m.\u001b[39mpolarity_scores)\n\u001b[1;32m---> 21\u001b[0m     headline, sentiment_vader \u001b[38;5;241m=\u001b[39m \u001b[43mobj_vader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# print(\"headlines:\")\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# print(headline.head())\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregated sentiment for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentiment_vader)\n",
      "Cell \u001b[1;32mIn[2], line 78\u001b[0m, in \u001b[0;36mHistoricalSentiment.calculate_sentiment\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_sentiment\u001b[39m(\u001b[38;5;28mself\u001b[39m, url):\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnews_scraped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnews_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_news_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnews_scraped)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# requires that find_articles has been called and generated a news_df\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m, in \u001b[0;36mHistoricalSentiment.find_articles\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(url_req)\n\u001b[0;32m     26\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url\u001b[38;5;241m=\u001b[39murl_req, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFireFox\u001b[39m\u001b[38;5;124m\"\u001b[39m}) \u001b[38;5;66;03m# I realize that aditya's version of the code doesn't use the right user agent\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m html \u001b[38;5;241m=\u001b[39m BeautifulSoup(response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m news_table \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews-table\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# id\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    530\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 531\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 640\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    562\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 563\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:755\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    752\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    753\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    522\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    524\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 525\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    528\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:542\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    541\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 542\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    543\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:1397\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\25ben\\anaconda3\\envs\\stocks_stats\\lib\\urllib\\request.py:1357\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1355\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1356\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1357\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1358\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>"
     ]
    }
   ],
   "source": [
    "import time\n",
    "with open(\"finvizurls_dates.txt\", \"r\") as f:\n",
    "    urls_select = f.readlines()\n",
    "\n",
    "# urls_select = random.sample(range(len(urls)), 40)\n",
    "# urls_select = [urls[i] for i in urls_select]\n",
    "news_tables = {}\n",
    "\n",
    "tickers = pd.read_csv(\"s&p_constituents.csv\")['Symbol'].tolist()\n",
    "\n",
    "print(\"urls selected\", len(urls_select))\n",
    "\n",
    "sentiments = {}\n",
    "for idx, url in enumerate(urls_select):\n",
    "    # save url to access metadata\n",
    "    # beatiful soup to extract the text\n",
    "    url = url.strip()\n",
    "    print(f\"=== URL {idx} ===\")\n",
    "    for ticker in tickers:\n",
    "        obj_vader = HistoricalSentiment(ticker, vader.polarity_scores)\n",
    "        headline, sentiment_vader = obj_vader.calculate_sentiment(url=url)\n",
    "        # print(\"headlines:\")\n",
    "        # print(headline.head())\n",
    "        print(f\"aggregated sentiment for {ticker}:\", sentiment_vader)\n",
    "        sentiments[ticker] = sentiment_vader\n",
    "        break\n",
    "\n",
    "    # sleep for a little for each url\n",
    "    time.sleep(3)\n",
    "\n",
    "print(\"sentiments\", sentiments)\n",
    "\n",
    "\n",
    "# TODO: investigate after hours stock moving - how do we deal with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your job is to determine the sentiment (positive, negative, neutral) corresponding\n",
    "    to companies and their stock tickers, if any, explicitly mentioned in the given financial news headline. \n",
    "    Only determine sentiment corresponding to companies explicitly mentioned in the \n",
    "    headline, do not try to predict companies that might be in the article. \n",
    "    Print only what belongs in the braces. \n",
    "    Examples of outputs include {st}. Do not explain the output.\n",
    "    If you are not sure, please don't share false information. \n",
    "\n",
    "    Headline: [Meet the Supercharged Growth Stock That's a Shoo-in to Join Microsoft in the $3 Trillion Club]\n",
    "    Predicted sentiment: {\"{}\"}\n",
    "    \n",
    "    di = re.search(r'\\{.*\\}', val).group()\n",
    "    # if the dictionary is not in the correct format, ensure the keys and values have quotes\n",
    "    # convert any company names to corresponding ticker\n",
    "    for name_idx, name in enumerate(names):\n",
    "        for key in di:\n",
    "            # if the key is a company name, replace it with the corresponding ticker\n",
    "            # name will be multiple words long. if any part of the name is in the key, replace it\n",
    "            for word in name.split():\n",
    "                if word.lower() in key:\n",
    "                    di = di.replace(key, tickers[name_idx])\n",
    "    # remove any quotes around the keys\n",
    "    # di = re.sub(r'\"', '', di)\n",
    "    di = re.sub(r'(\\w+):', r'\"\\1\":', di)\n",
    "\n",
    "    print(\"di original\", di)\n",
    "    di_real = eval(di)\n",
    "    print(headline, di_real)\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks_stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
