{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from waybackpy import WaybackMachineSaveAPI, WaybackMachineCDXServerAPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "# read from finvizurls.txt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://web.archive.org/web/20240000000000*/finviz.com\n",
    "tickers = [\"MSFT\"]\n",
    "web_url = \"https://finviz.com\" # /quote.ashx?t=\n",
    "# user_agent = \"my new app's user agent\"\n",
    "# start timestamp is 2 years ago\n",
    "cdx_api = WaybackMachineCDXServerAPI(web_url, start_timestamp=2021, end_timestamp=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in cdx_api.snapshots():\n",
    "    print(item.archive_url) # I just copied and pasted into finvizurls.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# not just recognizing sentiment, but also recognizing whether microsoft is actually in it or not? Filtering based on names? Downweighting\n",
    "# those that don't have msft in it?\n",
    "random.seed(0) # deterministic\n",
    "news_tables = {}\n",
    "\n",
    "with open(\"finvizurls.txt\", \"r\") as f:\n",
    "    urls = f.readlines()\n",
    "    # randomly generate 40 urls to save\n",
    "    urls_select = random.sample(range(len(urls)), 40)\n",
    "    for url in urls_select:\n",
    "        # save url to access metadata\n",
    "        # beatiful soup to extract the text\n",
    "        \"\"\"\n",
    "        eq = Request(url=url, headers={\"User-Agent\": \"FireFox\"})\n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response, \"html.parser\")\n",
    "        news_table = html.find(id='news-table')\n",
    "        news_tables[tick] = news_table\n",
    "        \"\"\"\n",
    "        url = url.strip()\n",
    "        for ticker in tickers:\n",
    "            url_req = f\"{url}/quote.ashx?t={ticker}\"\n",
    "            req = Request(url=url_req, headers={\"User-Agent\": \"FireFox\"}) # I realize that aditya's version of the code doesn't use the right user agent\n",
    "            response = urlopen(req)\n",
    "            html = BeautifulSoup(response, \"html.parser\")\n",
    "            news_table = html.find(id='news-table')\n",
    "            news_tables[ticker] = news_table\n",
    "            # print(\"news table\", news_table)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_tables[\"MSFT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "news_list = []\n",
    "\n",
    "for file_name, news_table in news_tables.items():\n",
    "    for i in news_table.findAll('tr'):\n",
    "        try:\n",
    "            text = i.a.get_text()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        date_scrape = i.td.text.split()\n",
    "        source = i.div.span.get_text()\n",
    "\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "\n",
    "        else:\n",
    "            final_date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "            if final_date == \"Today\":\n",
    "                final_date = date.today().strftime(\"%Y-%m-%d\") # b d y??\n",
    "\n",
    "        tick = file_name.split('_')[0]\n",
    "\n",
    "        news_list.append([tick, final_date, time, source, text])\n",
    "\n",
    "columns = ['ticker', 'date', 'time', 'source', 'headline']\n",
    "news_df = pd.DataFrame(news_list, columns=columns)\n",
    "news_df['date'] = pd.to_datetime(news_df.date, format='mixed').dt.date # format mixed??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news df    ticker        date     time                      source  \\\n",
      "95   MSFT  2020-12-22  02:08PM                Investopedia   \n",
      "96   MSFT  2020-12-22  02:05PM                Investopedia   \n",
      "97   MSFT  2020-12-22  02:01PM                Investopedia   \n",
      "98   MSFT  2020-12-22  12:13PM   Investor's Business Daily   \n",
      "99   MSFT  2020-12-22  12:00PM               TheStreet.com   \n",
      "\n",
      "                                             headline  \n",
      "95                            Companies Owned by MSFT  \n",
      "96                         Top Microsoft Shareholders  \n",
      "97  How Microsoft Makes Money: Personal Computing,...  \n",
      "98  Dow Jones Drops Despite Apple Stock's Gain; Tr...  \n",
      "99                    Does Microsoft Pass Our 'Test'?  \n"
     ]
    }
   ],
   "source": [
    "print(\"news df\", news_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker                                                   MSFT\n",
      "date                                               2020-12-31\n",
      "time                                                  05:03PM\n",
      "source                                            Barrons.com\n",
      "headline    Section 230 Keeps Coming Up in Congress. Heres...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(news_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vader 0.0844\n"
     ]
    }
   ],
   "source": [
    "# compare vader polarity vs our version\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_scores(df):\n",
    "    scores = df['headline'].apply(vader.polarity_scores).tolist()\n",
    "    scores = [x['compound'] for x in scores]\n",
    "    sentiment = float(np.mean(scores))\n",
    "    final_sentiment = round(sentiment, 4)\n",
    "    return final_sentiment\n",
    "\n",
    "print(\"vader\", get_scores(news_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks_stats_3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
