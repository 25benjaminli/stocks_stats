import csv
import requests

def fetch_and_paginate_news(start_date):
    all_news_data = []
    next_page_token = None

    while True:
        headers = {
            "accept": "application/json",
            "APCA-API-KEY-ID": "PKCWWRQXRDUHA39IOI8N",
            "APCA-API-SECRET-KEY": "5kSov913hC4g1U0YZxBwUrNf98vO4lKMlcrlqavv"
        }

        url = "https://data.alpaca.markets/v1beta1/news"
        params = {
            "start_date": start_date,
            "limit": 50,
            "include_content": True,
            "exclude_contentless": False,
            "sort" : "asc",
            "symbols" : "MSFT%2CAAPL%2CNVDA%2CGOOG%2CGOOGL%2CAMZN%2CMETA%2CBRK-B%2CLLY%2CAVGO%2CJPM%2CTSLA%2CV%2CWMT%2CXOM%2CUNH%2CMA%2CPG%2CCOST%2CJNJ%2CORCL%2CMRK%2CHD%2CBAC%2CCVX%2CNFLX%2CABBV%2CAMD%2CKO%2CCRM%2CPEP%2CQCOM%2CTMO%2CADBE%2CWFC%2CLIN%2CDHR%2CTMUS%2CACN%2CCSCO%2CMCD%2CDIS%2CGE%2CAMAT%2CTXN%2CABT%2CAXP%2CCAT%2CINTU%2CVZ%2CAMGN%2CPFE%2CMS%2CNEE%2CIBM%2CPM%2CNOW%2CCMCSA%2CBX%2CGS%2CISRG%2CMU%2CUNP%2CRTX%2CNKE%2CCOP%2CSPGI%2CETN%2CUBER%2CSCHW%2CINTC%2CHON%2CBKNG%2CSYK%2CLRCX%2CT%2CLOW%2CC%2CELV%2CPGR%2CUPS%2CVRTX%2CBLK%2CTJX%2CADI%2CLMT%2CBSX%2CREGN%2CMDT%2CCB%2CBA%2CKLAC%2CDE%2CPANW%2CMMC%2CADP%2CPLD%2CANET%2CCI%2CABNB%2CMDLZ%2CSNPS%2CSBUX%2CFI%2CAMT%2CCMG%2CSO%2CWM%2CBMY%2CHCA%2CGD%2CGILD%2CAPH%2CCDNS%2CDUK%2CICE%2CMO%2CZTS%2CCME%2CSHW%2CCL%2CTT%2CTDG%2CMCO%2CFCX%2CMCK%2CEQIX%2CCEG%2CITW%2CNXPI%2CEOG%2CCVS%2CCTAS%2CNOC%2CPH%2CMAR%2CTGT%2CECL%2CBDX%2CSLB%2CCSX%2CEMR%2CPYPL%2CMRNA%2CUSB%2CMPC%2CPNC%2CAON%2CFDX%2CMSI%2CPSX%2CWELL%2CCARR%2CRSG%2CAPD%2CROP%2CORLY%2CPCAR%2CSPG%2CMNST%2CMMM%2CAJG%2COXY%2CVLO%2CEW%2CMCHP%2CCOF%2CCPRT%2CAIG%2CSMCI%2CMET%2CTFC%2CNSC%2CHLT%2CDXCM%2CGM%2CAFL%2CJCI%2CWMB%2CTRV%2CGEV%2CNEM%2CF%2CPCG%2CSRE%2CAZO%2CPSA%2CROST%2CDHI%2CGWW%2CDLR%2COKE%2CAEP%2CFTNT%2CHES%2CKDP%2CTEL%2CADSK%2CSTZ%2CO%2CURI%2CEL%2CPAYX%2CKMB%2CD%2CBK%2CA%2CAMP%2CCOR%2CKHC%2CALL%2CFIS%2CLEN%2CPRU%2CIDXX%2CCCI%2CLHX%2CKMI%2CHUM%2CIQV%2CPWR%2CNUE%2CDOW%2CAME%2CHSY%2CCNC%2COTIS%2CCHTR%2CMSCI%2CCMI%2CACGL%2CYUM%2CCTVA%2CGIS%2CKR%2CIR%2CRCL%2CLULU%2CFAST%2CODFL%2CPEG%2CKVUE%2CEXC%2CMPWR%2CGEHC%2CSYY%2CEA%2CVRSK%2CNDAQ%2CMLM%2CVST%2CCSGP%2CXYL%2CHWM%2CFANG%2CVMC%2CFICO%2CIT%2CCTSH%2CDD%2CDAL%2CLVS%2CED%2CBKR%2CLYB%2CHPQ%2CHAL%2CDG%2CBIIB%2CMTD%2CGRMN%2CEXR%2CRMD%2CON%2CGLW%2CCDW%2CDFS%2CPPG%2CDVN%2CTSCO%2CROK%2CHIG%2CWAB%2CADM%2CXEL%2CEFX%2CFSLR%2CVICI%2CANSS%2CEIX%2CAVB%2CEBAY%2CCBRE%2CFTV%2CDECK%2CTTWO%2CTROW%2CGPN%2CRJF%2CCHD%2CWTW%2CBRO%2CWEC%2CFITB%2CTRGP%2CDOV%2CVLTO%2CDLTR%2CKEYS%2CMTB%2CAWK%2CEQR%2CIFF%2CWDC%2CWST%2CPHM%2CZBH%2CHPE%2CNTAP%2CBR%2CIRM%2CCAH%2CDTE%2CETR%2CNVR%2CSTT%2CSTE%2CTER%2CAPTV%2CFE%2CROL%2CLYV%2CHUBB%2CWY%2CPTC%2CBF-B%2CAXON%2CBALL%2CTSN%2CPPL%2CINVH%2CSTLD%2CTYL%2CBLDR%2CK%2CARE%2CLDOS%2CWRB%2CES%2CGPC%2CSBAC%2CCTRA%2CWAT%2CHBAN%2CCCL%2CSTX%2CMOH%2CMKC%2CPFG%2CALGN%2CHRL%2CVTR%2CCBOE%2CTDY%2CAEE%2CWBD%2CCNP%2CCOO%2CCPAY%2COMC%2CCINF%2CULTA%2CCMS%2CAVY%2CNRG%2CEQT%2CDRI%2CJ%2CDPZ%2CRF%2CSYF%2CBAX%2CESS%2CHOLX%2CVRSN%2CNTRS%2CENPH%2CEG%2CUAL%2CATO%2CILMN%2CTXT%2CLH%2CZBRA%2CCE%2CEXPD%2CFDS%2CL%2CPKG%2CCLX%2CIEX%2CJBHT%2CCFG%2CLUV%2CMAA%2CIP%2CDGX%2CGEN%2CBBY%2CNWS%2CNWSA%2CMAS%2CFOX%2CFOXA%2CALB%2CAES%2CSWKS%2CBG%2CUDR%2CEXPE%2CCAG%2CJBL%2CMRO%2CAMCR%2CAKAM%2CSNA%2CCF%2CWRK%2CRVTY%2CPOOL%2C\
                    TRMB%2CWBA%2CPNR%2CKEY%2CNDSN%2CCPB%2CDOC%2CSWK%2CHST%2CINCY%2CLW%2CLNT%2CTECH%2CPODD%2CNI%2CMGM%2CKIM%2CAOS%2CVTRS%2CJKHY%2CEVRG%2CBEN%2CDVA%2CIPG%2CUHS%2CEMN%2CSJM%2CLKQ%2CTAP%2CJNPR%2CCRL%2CCPT%2CREG%2CKMX%2CAPA%2CRL%2CBBWI%2CALLE%2CWYNN%2CBXP%2CEPAM%2CSOLV%2CCHRW%2CHII%2CFFIV%2CMOS%2CCTLT%2CPAYC%2CTFX%2CTPR%2CQRVO%2CHSIC%2CAAL%2CGNRC%2CDAY%2CAIZ%2CPNW%2CHAS%2CPARA%2CMKTX%2CFRT%2CBIO%2CBWA%2CMTCH%2CFMC%2CGL%2CMHK%2CCZR%2CETSY%2CIVZ"
        }
        if next_page_token:
            url += "&page_token=" + next_page_token

        response = requests.get(url, params=params, headers=headers)
        print(url)
        data = response.json()

        if 'news' not in data:
            print("error1")
            break

        all_news_data.extend(data['news'])
        next_page_token = data.get('next_page_token')

        if not next_page_token:
            print("error2")
            break

    return all_news_data

# Read start dates from CSV
start_dates = []
with open('random_days.csv', newline='') as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
        start_dates.extend(row)

# Fetch and paginate news for each start date
for start_date in start_dates:
    print("Fetching news for start date:", start_date)
    news_data = fetch_and_paginate_news(start_date)
    # Print a summary of the news data
    print("Number of news articles:", len(news_data))
    if len(news_data) > 0:
        print("First headline:", news_data[0]['headline'])
